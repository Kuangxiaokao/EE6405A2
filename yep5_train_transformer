{"cells":[{"cell_type":"markdown","id":"8a77807f92f26ee","metadata":{"id":"8a77807f92f26ee"},"source":["# This is a sample Jupyter Notebook\n","\n","Below is an example of a code cell.\n","Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n","\n","Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n","\n","To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n","For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."]},{"cell_type":"code","execution_count":null,"id":"8b2e5954646de26f","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:13.539263Z","start_time":"2024-11-04T16:13:10.868970Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"8b2e5954646de26f","executionInfo":{"status":"ok","timestamp":1730889776556,"user_tz":-480,"elapsed":2387,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"138a25f6-4ab1-4fbc-b36b-631ea286a568"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cpu)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"]}],"source":["!pip install torch"]},{"cell_type":"code","execution_count":null,"id":"ac2ff69cbf8f794f","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:26.669567Z","start_time":"2024-11-04T16:13:22.146089Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ac2ff69cbf8f794f","executionInfo":{"status":"ok","timestamp":1730889782927,"user_tz":-480,"elapsed":3403,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"4b267bdd-4b30-408d-97a5-7ab281fa4488"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (18.0.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"]}],"source":["!pip install pyarrow\n","!pip install keras"]},{"cell_type":"code","execution_count":null,"id":"fbc121e30a2defb3","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:35.457027Z","start_time":"2024-11-04T16:13:29.079779Z"},"id":"fbc121e30a2defb3"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import math\n","import copy\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","import numpy as np\n","import pyarrow.parquet as pq\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.layers import TextVectorization, Embedding, Dropout, Dense, LayerNormalization, MultiHeadAttention, Input\n","from tensorflow.keras import Sequential, callbacks, Model\n"]},{"cell_type":"code","execution_count":null,"id":"cb735bc0712cf7ce","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:36.877325Z","start_time":"2024-11-04T16:13:36.863325Z"},"id":"cb735bc0712cf7ce"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","\n","    \"\"\"\n","    The init constructor checks whether the provided d_model is divisible by the number of heads (num_heads).\n","    It sets up the necessary parameters and creates linear transformations for\n","    query(W_q), key(W_k) and output(W_o) projections\n","    \"\"\"\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","    \"\"\"\n","     The scaled_dot_product_attention function computes the scaled dot-product attention given the\n","     query (Q), key (K), and value (V) matrices. It uses the scaled dot product formula, applies a mask if\n","     provided, and computes the attention probabilities using the softmax function.\n","    \"\"\"\n","    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n","        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","        if mask is not None:\n","            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n","        attn_probs = torch.softmax(attn_scores, dim=-1)\n","        output = torch.matmul(attn_probs, V)\n","        return output\n","\n","    \"\"\"\n","    The split_heads and combine_heads functions handle the splitting and combining of the attention heads.\n","    They reshape the input tensor to allow parallel processing of different attention heads.\n","    \"\"\"\n","    def split_heads(self, x):\n","        batch_size, seq_length, d_model = x.size()\n","        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n","\n","    def combine_heads(self, x):\n","        batch_size, _, seq_length, d_k = x.size()\n","        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n","\n","    \"\"\"\n","     The forward function takes input query (Q), key (K), and value (V) tensors,\n","     applies linear transformations, splits them into multiple heads, performs scaled dot-product attention,\n","     combines the attention heads, and applies a final linear transformation.\n","    \"\"\"\n","    def forward(self, Q, K, V, mask=None):\n","        Q = self.split_heads(self.W_q(Q))\n","        K = self.split_heads(self.W_k(K))\n","        V = self.split_heads(self.W_v(V))\n","\n","        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n","        output = self.W_o(self.combine_heads(attn_output))\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"40571ca4bd61ea97","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:40.123458Z","start_time":"2024-11-04T16:13:40.118264Z"},"id":"40571ca4bd61ea97"},"outputs":[],"source":["class PositionWiseFeedForward(nn.Module):\n","    \"\"\"\n","    PositionWiseFeedForward module. It takes d_model as the input dimension and d_ff\n","    as the hidden layer dimension.\n","    Two linear layers (fc1 and fc2) are defined with ReLU activation in between.\n","    \"\"\"\n","    def __init__(self, d_model, d_ff):\n","        super(PositionWiseFeedForward, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","        self.relu = nn.ReLU()\n","\n","    \"\"\"\n","    The forward function takes an input tensor x, applies the first linear transformation (fc1),\n","    applies the ReLU activation, and then applies the second linear transformation (fc2).\n","    The output is the result of the second linear transformation.\n","    \"\"\"\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"]},{"cell_type":"code","execution_count":null,"id":"e17f5bbcf7a7a30b","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:42.454Z","start_time":"2024-11-04T16:13:42.446614Z"},"id":"e17f5bbcf7a7a30b"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    The constructor (__init__) initializes the PositionalEncoding module.\n","    It takes d_model as the dimension of the model and max_seq_length as the maximum sequence length.\n","    It computes the positional encoding matrix (pe) using sine and cosine functions.\n","    \"\"\"\n","    def __init__(self, d_model, max_seq_length):\n","        super(PositionalEncoding, self).__init__()\n","\n","        pe = torch.zeros(max_seq_length, d_model)\n","        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    \"\"\"\n","    The forward function takes an input tensor x and adds the positional encoding to it.\n","    The positional encoding is truncated to match the length of the input sequence (x.size(1)).\n","    \"\"\"\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]"]},{"cell_type":"code","execution_count":null,"id":"7d5de3feb4421a44","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:44.366626Z","start_time":"2024-11-04T16:13:44.355546Z"},"id":"7d5de3feb4421a44"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","\n","    \"\"\"\n","    The constructor (__init__) initializes the EncoderLayer module.\n","    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads),\n","    d_ff (dimension of the feedforward network), and dropout (dropout rate).\n","    It creates instances of MultiHeadAttention, PositionWiseFeedForward, and nn.LayerNorm.\n","    Dropout is also defined as a module.\n","    \"\"\"\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    \"\"\"\n","    The forward function takes an input tensor x and a mask.\n","    It applies the self-attention mechanism (self.self_attn), adds the residual connection\n","    with layer normalization, applies the position-wise feedforward network (self.feed_forward),\n","    and again adds the residual connection with layer normalization.\n","    Dropout is applied at both the self-attention and feedforward stages.\n","    The mask parameter is used to mask certain positions during the self-attention step,\n","    typically to prevent attending to future positions in a sequence.\n","    \"\"\"\n","    def forward(self, x, mask):\n","        attn_output = self.self_attn(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"1e574e9ee33627ca","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:46.566155Z","start_time":"2024-11-04T16:13:46.558193Z"},"id":"1e574e9ee33627ca"},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \"\"\"\n","    The constructor (__init__) initializes the DecoderLayer module.\n","    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads),\n","    d_ff (dimension of the feedforward network), and dropout (dropout rate).\n","    It creates instances of MultiHeadAttention for both self-attention (self.self_attn) and cross-attention\n","    (self.cross_attn), PositionWiseFeedForward, and nn.LayerNorm. Dropout is also defined as a module.\n","    \"\"\"\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n","        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    \"\"\"\n","    The forward function takes an input tensor x, the output from the encoder (enc_output),\n","    and masks for the source (src_mask) and target (tgt_mask). It applies the self-attention mechanism,\n","    adds the residual connection with layer normalization, applies the cross-attention mechanism with the\n","    encoder's output, adds another residual connection with layer normalization, applies the position-wise\n","    feedforward network, and adds a final residual connection with layer normalization.\n","    Dropout is applied at each stage.\n","    \"\"\"\n","    def forward(self, x, enc_output, src_mask, tgt_mask):\n","        attn_output = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n","        x = self.norm2(x + self.dropout(attn_output))\n","        ff_output = self.feed_forward(x)\n","        x = self.norm3(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"ceb76f270903094b","metadata":{"ExecuteTime":{"end_time":"2024-11-04T16:13:48.646769Z","start_time":"2024-11-04T16:13:48.638554Z"},"id":"ceb76f270903094b"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n","        super(Transformer, self).__init__()\n","        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","\n","        self.fc = nn.Linear(d_model, 5)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def generate_mask(self, src, tgt):\n","        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n","        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n","        seq_length = tgt.size(1)\n","        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n","        tgt_mask = tgt_mask & nopeak_mask\n","        return src_mask, tgt_mask\n","\n","    def forward(self, src, tgt):\n","        src_mask, tgt_mask = self.generate_mask(src, tgt)\n","        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n","        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n","\n","        enc_output = src_embedded\n","        for enc_layer in self.encoder_layers:\n","            enc_output = enc_layer(enc_output, src_mask)\n","\n","        dec_output = tgt_embedded\n","        for dec_layer in self.decoder_layers:\n","            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n","\n","        # 使用解码器输出的第一个token (假设这是[CLS]位置) 来进行句子级别情感分类\n","        cls_token_output = dec_output[:, 0, :]  # 获取第一个token的输出\n","        output = self.fc(cls_token_output)\n","        return output"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQ2CpmmZAa-s","executionInfo":{"status":"ok","timestamp":1730889835466,"user_tz":-480,"elapsed":18175,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"c206b3e0-acec-44ae-9e6e-615b8ae93d41"},"id":"qQ2CpmmZAa-s","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"05c2a9d8-ca83-46a9-8eb8-f9e264c82b17","metadata":{"id":"05c2a9d8-ca83-46a9-8eb8-f9e264c82b17","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730890199260,"user_tz":-480,"elapsed":622,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"e635de5d-fb53-475c-fff3-cf6ddd2350c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Data - First 5 Rows:\n","        label                                               text\n","177288      0  first of all I m not a big fan of buffet I try...\n","238756      1  thanks yelp I be look for the word to describe...\n","604225      2  service be so so they be receive a delivery so...\n","2838        2  stamoolis brother be one of the strip district...\n","586957      0  I want to give a star because the service staf...\n","\n","Validation Data - First 5 Rows:\n","       label                                               text\n","33553      4  come a few day ago for a lease wasn t sure of ...\n","9427       0  I choose the queen for my visit to las vegas f...\n","199        3  I go here on the day of a wedding I m from out...\n","12447      1  isn t it strange how the little thing can sour...\n","39489      4  visit here several time a year the food be alw...\n","\n","Test Data - First 5 Rows:\n","       label                                               text\n","33553      4  come a few day ago for a lease wasn t sure of ...\n","9427       0  I choose the queen for my visit to las vegas f...\n","199        3  I go here on the day of a wedding I m from out...\n","12447      1  isn t it strange how the little thing can sour...\n","39489      4  visit here several time a year the food be alw...\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","import pyarrow.parquet as pq\n","\n","# 加载数据集\n","def LoadDataset(file_name):\n","    parquet_file = pq.ParquetFile(file_name)\n","    data = parquet_file.read().to_pandas()\n","    return data\n","\n","train_data = LoadDataset('/content/drive/MyDrive/Colab Notebooks/data/yelp5_process/train.parquet')\n","validation_data = LoadDataset('/content/drive/MyDrive/Colab Notebooks/data/yelp5_process/test.parquet')\n","test_data = LoadDataset('/content/drive/MyDrive/Colab Notebooks/data/yelp5_process/test.parquet')\n","# 打印前五行\n","print(\"Train Data - First 5 Rows:\")\n","print(train_data.head())\n","\n","print(\"\\nValidation Data - First 5 Rows:\")\n","print(validation_data.head())\n","\n","print(\"\\nTest Data - First 5 Rows:\")\n","print(test_data.head())\n","\n","def preprocess_text(text, max_len=128):\n","    token_ids = [ord(char) for char in text[:max_len]]\n","    return token_ids + [0] * (max_len - len(token_ids))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"314174c3-e5aa-467b-9e9e-2e9d0c95fdbd","metadata":{"id":"314174c3-e5aa-467b-9e9e-2e9d0c95fdbd"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","\n","# Custom loss function for labels in range 0-4\n","def custom_loss(outputs, labels):\n","    # Standard CrossEntropyLoss since there’s no need to ignore any index\n","    return nn.CrossEntropyLoss()(outputs, labels)\n","\n","# Custom dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, data, max_len=128):\n","        self.texts = data['text'].values\n","        self.labels = data['label'].values.astype(int)\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        token_ids = preprocess_text(text, self.max_len)\n","        return torch.tensor(token_ids), torch.tensor(label, dtype=torch.long)\n","\n","# Initialize datasets and loaders\n","train_dataset = TextDataset(train_data)\n","val_dataset = TextDataset(validation_data)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, num_epochs=30, learning_rate=1e-4, patience=3):\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","\n","    train_losses, val_losses = [], []\n","    train_accuracies, val_accuracies = [], []\n","\n","    best_val_loss = float('inf')\n","    early_stop_count = 0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        train_preds, train_labels = [], []\n","\n","        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n","            optimizer.zero_grad()\n","            outputs = model(inputs, inputs)\n","\n","            # Use custom loss function\n","            loss = custom_loss(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_preds.extend(torch.argmax(outputs, dim=-1).cpu().numpy().flatten())\n","            train_labels.extend(labels.cpu().numpy().flatten())\n","\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Compute training accuracy\n","        train_accuracy = accuracy_score(train_labels, train_preds)\n","        train_accuracies.append(train_accuracy)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        val_preds, val_labels = [], []\n","\n","        with torch.no_grad():\n","            for inputs, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n","                outputs = model(inputs, inputs)\n","                loss = custom_loss(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n","                val_loss += loss.item()\n","\n","                val_preds.extend(torch.argmax(outputs, dim=-1).cpu().numpy().flatten())\n","                val_labels.extend(labels.cpu().numpy().flatten())\n","\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        # Compute validation accuracy\n","        val_accuracy = accuracy_score(val_labels, val_preds)\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, '\n","              f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.4f}, '\n","              f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            early_stop_count = 0\n","        else:\n","            early_stop_count += 1\n","            if early_stop_count >= patience:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","                break\n","\n","        scheduler.step()\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies\n"]},{"cell_type":"code","execution_count":null,"id":"9ce79cd0-9857-4107-bfae-743bade386f3","metadata":{"editable":true,"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"9ce79cd0-9857-4107-bfae-743bade386f3","executionInfo":{"status":"error","timestamp":1730892924332,"user_tz":-480,"elapsed":14352,"user":{"displayName":"Hasem ndbxj Hdjdnv","userId":"12114095975536990774"}},"outputId":"08cf9ece-945e-47fb-dbea-e020af02216d"},"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-deca48e9ffd1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     num_layers=6, d_ff=2048, max_seq_length=128, dropout=0.1)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-93f59af5cdb2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, patience)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Use custom loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c7ab4bc5b673>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_embedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdec_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mdec_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 使用解码器输出的第一个token (假设这是[CLS]位置) 来进行句子级别情感分类\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-6ad6c648a284>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-4b80c1fe3d20>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = Transformer(src_vocab_size=1000, tgt_vocab_size=1000, d_model=512, num_heads=8,\n","                    num_layers=6, d_ff=2048, max_seq_length=128, dropout=0.1)\n","\n","train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, num_epochs=20)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n","plt.plot(val_accuracies, label='Validation Accuracy', color='yellow')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Training and Validation Accuracy per Epoch')\n","plt.savefig('accuracy_plot.png')\n","plt.show()\n","\n","\n","\n","np.savez('training_results.npz', train_losses=train_losses, val_losses=val_losses,\n","         train_accuracies=train_accuracies, val_accuracies=val_accuracies)"]},{"cell_type":"code","execution_count":null,"id":"39df1bb0-188a-47e3-a8be-81a098e48389","metadata":{"editable":true,"tags":[],"id":"39df1bb0-188a-47e3-a8be-81a098e48389"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[{"file_id":"1qfFGew6a2KEHshs2dXNGxsedG_B9CuiR","timestamp":1730889744629}],"machine_shape":"hm","gpuType":"V28"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}